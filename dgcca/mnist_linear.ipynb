{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classical-cricket",
   "metadata": {},
   "source": [
    "## Full resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worthy-manor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_count</th>\n",
       "      <th>epoch</th>\n",
       "      <th>data_fidelity</th>\n",
       "      <th>val_fidelity</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006355</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0.003699</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.003699</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run_count  epoch  data_fidelity  val_fidelity  batch_size      lr device\n",
       "0           1      0       0.006355      0.000932        1000  0.0001    cpu\n",
       "1           1      1       0.003814      0.000948        1000  0.0001    cpu\n",
       "2           1      2       0.003763      0.000926        1000  0.0001    cpu\n",
       "3           1      3       0.003758      0.000908        1000  0.0001    cpu\n",
       "4           1      4       0.003750      0.000921        1000  0.0001    cpu\n",
       "5           1      5       0.003746      0.000906        1000  0.0001    cpu\n",
       "6           1      6       0.003740      0.000914        1000  0.0001    cpu\n",
       "7           1      7       0.003740      0.000894        1000  0.0001    cpu\n",
       "8           1      8       0.003736      0.000913        1000  0.0001    cpu\n",
       "9           1      9       0.003736      0.000902        1000  0.0001    cpu\n",
       "10          1     10       0.003731      0.000895        1000  0.0001    cpu\n",
       "11          1     11       0.003729      0.000894        1000  0.0001    cpu\n",
       "12          1     12       0.003729      0.000900        1000  0.0001    cpu\n",
       "13          1     13       0.003726      0.000885        1000  0.0001    cpu\n",
       "14          1     14       0.003727      0.000897        1000  0.0001    cpu\n",
       "15          1     15       0.003724      0.000893        1000  0.0001    cpu\n",
       "16          1     16       0.003725      0.000899        1000  0.0001    cpu\n",
       "17          1     17       0.003723      0.000904        1000  0.0001    cpu\n",
       "18          1     18       0.003723      0.000896        1000  0.0001    cpu\n",
       "19          1     19       0.003722      0.000902        1000  0.0001    cpu\n",
       "20          1     20       0.003720      0.000902        1000  0.0001    cpu\n",
       "21          1     21       0.003715      0.000880        1000  0.0001    cpu\n",
       "22          1     22       0.003716      0.000899        1000  0.0001    cpu\n",
       "23          1     23       0.003714      0.000900        1000  0.0001    cpu\n",
       "24          1     24       0.003712      0.000892        1000  0.0001    cpu\n",
       "25          1     25       0.003714      0.000883        1000  0.0001    cpu\n",
       "26          1     26       0.003713      0.000896        1000  0.0001    cpu\n",
       "27          1     27       0.003711      0.000902        1000  0.0001    cpu\n",
       "28          1     28       0.003712      0.000882        1000  0.0001    cpu\n",
       "29          1     29       0.003711      0.000897        1000  0.0001    cpu\n",
       "30          1     30       0.003708      0.000902        1000  0.0001    cpu\n",
       "31          1     31       0.003708      0.000889        1000  0.0001    cpu\n",
       "32          1     32       0.003703      0.000893        1000  0.0001    cpu\n",
       "33          1     33       0.003704      0.000904        1000  0.0001    cpu\n",
       "34          1     34       0.003702      0.000895        1000  0.0001    cpu\n",
       "35          1     35       0.003701      0.000892        1000  0.0001    cpu\n",
       "36          1     36       0.003703      0.000897        1000  0.0001    cpu\n",
       "37          1     37       0.003704      0.000892        1000  0.0001    cpu\n",
       "38          1     38       0.003699      0.000882        1000  0.0001    cpu\n",
       "39          1     39       0.003697      0.000885        1000  0.0001    cpu\n",
       "40          1     40       0.003698      0.000895        1000  0.0001    cpu\n",
       "41          1     41       0.003699      0.000883        1000  0.0001    cpu\n",
       "42          1     42       0.003697      0.000889        1000  0.0001    cpu\n",
       "43          1     43       0.003698      0.000880        1000  0.0001    cpu\n",
       "44          1     44       0.003697      0.000879        1000  0.0001    cpu\n",
       "45          1     45       0.003698      0.000884        1000  0.0001    cpu\n",
       "46          1     46       0.003701      0.000889        1000  0.0001    cpu\n",
       "47          1     47       0.003696      0.000891        1000  0.0001    cpu\n",
       "48          1     48       0.003693      0.000888        1000  0.0001    cpu\n",
       "49          1     49       0.003690      0.000891        1000  0.0001    cpu\n",
       "50          1     50       0.003691      0.000888        1000  0.0001    cpu\n",
       "51          1     51       0.003691      0.000888        1000  0.0001    cpu\n",
       "52          1     52       0.003689      0.000890        1000  0.0001    cpu\n",
       "53          1     53       0.003691      0.000889        1000  0.0001    cpu\n",
       "54          1     54       0.003688      0.000890        1000  0.0001    cpu\n",
       "55          1     55       0.003688      0.000894        1000  0.0001    cpu\n",
       "56          1     56       0.003687      0.000889        1000  0.0001    cpu\n",
       "57          1     57       0.003688      0.000884        1000  0.0001    cpu\n",
       "58          1     58       0.003686      0.000892        1000  0.0001    cpu\n",
       "59          1     59       0.003687      0.000894        1000  0.0001    cpu"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset import get_mnist_dataset\n",
    "train_set, train_classes = get_mnist_dataset(train=True, shuffle=True, normalize=False)\n",
    "val_set, val_classes = get_mnist_dataset(train=False, total_data=1000, shuffle=True, normalize=False)\n",
    "\n",
    "train_set = train_set.view(train_set.shape[0], train_set.shape[1], -1)\n",
    "val_set = val_set.view(val_set.shape[0], val_set.shape[1], -1)\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import product\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.run_manager import RunBuilder\n",
    "from models import g_step, MnistAEDGCCA, MnistAELinear\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from dataset import get_mnist_dataset \n",
    "import numpy as np\n",
    "\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    devices = ['cpu']\n",
    "else:\n",
    "    devices = ['cpu']\n",
    "print('starting')\n",
    "\n",
    "\n",
    "params = OrderedDict(\n",
    "    lr = [0.0001],\n",
    "    batch_size = [1000],\n",
    "    device = devices,\n",
    "    shuffle = [True],\n",
    "    num_workers = [5],\n",
    "    manual_seed = [1265],\n",
    "    loss_func = [nn.MSELoss],\n",
    "    quant = [True],\n",
    "    latent_dim = [10], \n",
    "    num_inner_epochs = [1],\n",
    "    n_bits = [2]\n",
    ")\n",
    "\n",
    "# layer_sizes_list = 3*[[128, 64, 2]]\n",
    "# input_size_list = 3*[2]\n",
    "\n",
    "\n",
    "run_count = 0\n",
    "models = []\n",
    "\n",
    "\n",
    "run_data = []\n",
    "\n",
    "data_load_time = 0\n",
    "forward_time = 0\n",
    "\n",
    "\n",
    "for run in RunBuilder.get_runs(params):\n",
    "#     torch.cuda.set_device(run.device)\n",
    "    \n",
    "    run_count += 1\n",
    "    device = torch.device(run.device)\n",
    "    \n",
    "    dgcca = MnistAEDGCCA(output_size=run.latent_dim, network=MnistAELinear)\n",
    "    dgcca = dgcca.to('cpu')\n",
    "        \n",
    "    train_views = list(train_set.to('cpu'))\n",
    "    val_views = list(val_set.to('cpu'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(dgcca.parameters(), lr=run.lr)\n",
    "    num_batches = len(train_views[0])//run.batch_size\n",
    "    \n",
    "    criterion = run.loss_func()\n",
    "\n",
    "    out = torch.stack(dgcca(train_views))\n",
    "    G = g_step(out.clone().detach())  \n",
    "    \n",
    "    M_serv = out.detach().clone()\n",
    "    \n",
    "    I = len(train_views)\n",
    "    \n",
    "    for epoch in range(60):\n",
    "        total_recons_loss = 0\n",
    "        total_val_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        dgcca.train()\n",
    "        dgcca.to('cuda')\n",
    "        \n",
    "        for _ in trange(run.num_inner_epochs):\n",
    "            for i in trange(num_batches):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch = []\n",
    "\n",
    "                # mini batch gradient\n",
    "                batch = [view[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda') for view in train_views]            \n",
    "                target = G[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda')\n",
    "\n",
    "                latent = dgcca(batch)\n",
    "\n",
    "                ae_loss = (run.latent_dim/(2*28*28*target.shape[0]))*torch.norm(torch.stack(dgcca.decode(latent)) - torch.stack(batch))\n",
    "    \n",
    "                dgcca_loss = 1/2*torch.norm(torch.stack(latent)-target)/target.shape[0] \n",
    "\n",
    "                loss = dgcca_loss + ae_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_recons_loss += loss.item()\n",
    "                del batch, target, latent\n",
    "\n",
    "        dgcca.eval()\n",
    "        dgcca.to('cpu')\n",
    "        out = torch.stack(dgcca(train_views)).detach().clone()        \n",
    "        if run.quant:\n",
    "            for i in range(I):\n",
    "                diff = out[i] - M_serv[i]\n",
    "                max_val = diff.abs().max()\n",
    "                quant = ((1/max_val)*diff[i]).round()*(max_val/1)\n",
    "                var = M_serv[i] + quant\n",
    "                M_serv[i] = var\n",
    "                del max_val, diff, quant, var\n",
    "            G = g_step(M_serv.clone().detach())\n",
    "        else:\n",
    "            G = g_step(out.clone().detach())   \n",
    "            \n",
    "        # validation loss\n",
    "        out_val = dgcca(val_views)\n",
    "        out_val = torch.stack(out_val)\n",
    "        \n",
    "        G_val = g_step(out_val.clone().detach())\n",
    "        \n",
    "        \n",
    "        loss_val = criterion(out_val, G_val)\n",
    "        total_val_loss += loss_val.item()\n",
    "        del out, G_val, out_val\n",
    "\n",
    "        \n",
    "        results = OrderedDict()\n",
    "        results['run_count'] = run_count\n",
    "        results['epoch'] = epoch\n",
    "        results['data_fidelity'] = total_recons_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['val_fidelity'] = total_val_loss\n",
    "        results['batch_size'] = run.batch_size\n",
    "        results['lr'] = run.lr\n",
    "        results['device'] = run.device\n",
    "        \n",
    "#         if results['data_fidelity'] < 0.00275:\n",
    "#             break\n",
    "        \n",
    "        run_data.append(results)\n",
    "        df3 = pd.DataFrame.from_dict(run_data, orient='columns')\n",
    "        clear_output(wait=True)\n",
    "#         show_latent()\n",
    "        display(df3)\n",
    "            \n",
    "#             m.track_loss(G_adv_loss=losses['beta_kl-divergence'], G_mse_loss=losses[''], D_real_loss=total_D_real, D_fake_loss=total_D_fake, D_real_count=real_count, D_fake_count=fake_count)\n",
    "#         print(epoch, \"total_Gloss:\",total_Gloss, \"total_Dloss:\",total_Dloss, \"mse:\",total_mse_loss, \"adv: \", total_adv_loss)           \n",
    "#         m.end_epoch()\n",
    "        torch.save(dgcca, 'trained_models/dgcca_mnist_linear5_innerit1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-burke",
   "metadata": {},
   "source": [
    "## Quantized DGCCA on MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prostate-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>total_loss</th>\n",
       "      <th>ae_loss</th>\n",
       "      <th>dgcca_loss</th>\n",
       "      <th>val_fidelity</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012688</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.009261</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.005636</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.003381</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.005163</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.005004</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.003525</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  total_loss   ae_loss  dgcca_loss  val_fidelity  batch_size     lr\n",
       "0       0    0.012688  0.003427    0.009261      0.003522        1000  0.001\n",
       "1       1    0.005458  0.003353    0.002105      0.003213        1000  0.001\n",
       "2       2    0.005437  0.003350    0.002087      0.002929        1000  0.001\n",
       "3       3    0.005254  0.003348    0.001905      0.002973        1000  0.001\n",
       "4       4    0.005636  0.003348    0.002288      0.002803        1000  0.001\n",
       "5       5    0.005409  0.003348    0.002062      0.002860        1000  0.001\n",
       "6       6    0.005572  0.003347    0.002225      0.002944        1000  0.001\n",
       "7       7    0.005564  0.003347    0.002216      0.002873        1000  0.001\n",
       "8       8    0.006022  0.003347    0.002674      0.003381        1000  0.001\n",
       "9       9    0.005640  0.003347    0.002292      0.002840        1000  0.001\n",
       "10     10    0.005518  0.003347    0.002171      0.003005        1000  0.001\n",
       "11     11    0.005393  0.003347    0.002046      0.002930        1000  0.001\n",
       "12     12    0.005163  0.003347    0.001816      0.002674        1000  0.001\n",
       "13     13    0.005377  0.003347    0.002030      0.002962        1000  0.001\n",
       "14     14    0.005480  0.003347    0.002133      0.002754        1000  0.001\n",
       "15     15    0.005348  0.003347    0.002001      0.002620        1000  0.001\n",
       "16     16    0.005364  0.003347    0.002017      0.003355        1000  0.001\n",
       "17     17    0.005345  0.003347    0.001998      0.003197        1000  0.001\n",
       "18     18    0.005004  0.003347    0.001657      0.002729        1000  0.001\n",
       "19     19    0.004603  0.003347    0.001256      0.002604        1000  0.001\n",
       "20     20    0.003882  0.003347    0.000535      0.002609        1000  0.001\n",
       "21     21    0.003611  0.003347    0.000264      0.002563        1000  0.001\n",
       "22     22    0.003590  0.003347    0.000244      0.002551        1000  0.001\n",
       "23     23    0.003583  0.003347    0.000237      0.002545        1000  0.001\n",
       "24     24    0.003579  0.003347    0.000232      0.002540        1000  0.001\n",
       "25     25    0.003574  0.003347    0.000227      0.002533        1000  0.001\n",
       "26     26    0.003570  0.003347    0.000223      0.002527        1000  0.001\n",
       "27     27    0.003566  0.003347    0.000219      0.002523        1000  0.001\n",
       "28     28    0.003563  0.003347    0.000216      0.002520        1000  0.001\n",
       "29     29    0.003560  0.003347    0.000214      0.002518        1000  0.001\n",
       "30     30    0.003558  0.003347    0.000211      0.002516        1000  0.001\n",
       "31     31    0.003555  0.003347    0.000208      0.002514        1000  0.001\n",
       "32     32    0.003552  0.003347    0.000205      0.002511        1000  0.001\n",
       "33     33    0.003549  0.003347    0.000202      0.002508        1000  0.001\n",
       "34     34    0.003546  0.003347    0.000200      0.002506        1000  0.001\n",
       "35     35    0.003544  0.003346    0.000197      0.002504        1000  0.001\n",
       "36     36    0.003541  0.003346    0.000194      0.002500        1000  0.001\n",
       "37     37    0.003536  0.003346    0.000190      0.002495        1000  0.001\n",
       "38     38    0.003532  0.003346    0.000185      0.002491        1000  0.001\n",
       "39     39    0.003528  0.003346    0.000182      0.002488        1000  0.001\n",
       "40     40    0.003525  0.003346    0.000179      0.002485        1000  0.001\n",
       "41     41    0.003523  0.003346    0.000177      0.002483        1000  0.001\n",
       "42     42    0.003521  0.003346    0.000175      0.002482        1000  0.001\n",
       "43     43    0.003520  0.003346    0.000173      0.002480        1000  0.001\n",
       "44     44    0.003518  0.003346    0.000172      0.002479        1000  0.001\n",
       "45     45    0.003517  0.003346    0.000170      0.002477        1000  0.001\n",
       "46     46    0.003515  0.003346    0.000169      0.002476        1000  0.001\n",
       "47     47    0.003513  0.003346    0.000167      0.002474        1000  0.001\n",
       "48     48    0.003510  0.003346    0.000164      0.002471        1000  0.001\n",
       "49     49    0.003508  0.003346    0.000161      0.002468        1000  0.001\n",
       "50     50    0.003505  0.003346    0.000158      0.002465        1000  0.001\n",
       "51     51    0.003501  0.003346    0.000155      0.002462        1000  0.001\n",
       "52     52    0.003499  0.003346    0.000152      0.002460        1000  0.001\n",
       "53     53    0.003496  0.003346    0.000150      0.002458        1000  0.001\n",
       "54     54    0.003494  0.003346    0.000148      0.002456        1000  0.001\n",
       "55     55    0.003493  0.003346    0.000146      0.002455        1000  0.001\n",
       "56     56    0.003491  0.003346    0.000145      0.002453        1000  0.001\n",
       "57     57    0.003489  0.003346    0.000143      0.002452        1000  0.001\n",
       "58     58    0.003487  0.003346    0.000140      0.002450        1000  0.001\n",
       "59     59    0.003483  0.003346    0.000137      0.002446        1000  0.001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset import get_mnist_dataset\n",
    "train_set, train_classes = get_mnist_dataset(train=True, shuffle=True, normalize=False)\n",
    "val_set, val_classes = get_mnist_dataset(train=False, total_data=1000, shuffle=True, normalize=False)\n",
    "\n",
    "train_set = train_set.view(train_set.shape[0], train_set.shape[1], -1)\n",
    "val_set = val_set.view(val_set.shape[0], val_set.shape[1], -1)\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import product\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.run_manager import RunBuilder\n",
    "from models import g_step, MnistAEDGCCA, MnistAELinear\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from dataset import get_mnist_dataset \n",
    "import numpy as np\n",
    "\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    devices = ['cpu']\n",
    "else:\n",
    "    devices = ['cpu']\n",
    "print('starting')\n",
    "\n",
    "\n",
    "params = OrderedDict(\n",
    "    lr = [0.001],\n",
    "    batch_size = [1000],\n",
    "    device = devices,\n",
    "    shuffle = [True],\n",
    "    num_workers = [5],\n",
    "    manual_seed = [1265],\n",
    "    loss_func = [nn.MSELoss],\n",
    "    quant = [False],\n",
    "    latent_dim = [10], \n",
    "    num_inner_epochs = [1]\n",
    ")\n",
    "\n",
    "# layer_sizes_list = 3*[[128, 64, 2]]\n",
    "# input_size_list = 3*[2]\n",
    "\n",
    "\n",
    "run_count = 0\n",
    "models = []\n",
    "\n",
    "\n",
    "run_data = []\n",
    "\n",
    "data_load_time = 0\n",
    "forward_time = 0\n",
    "\n",
    "\n",
    "for run in RunBuilder.get_runs(params):\n",
    "#     torch.cuda.set_device(run.device)\n",
    "    \n",
    "    run_count += 1\n",
    "    device = torch.device(run.device)\n",
    "    \n",
    "    dgcca = MnistAEDGCCA(output_size=run.latent_dim, network=MnistAELinear)\n",
    "    dgcca = dgcca.to('cpu')\n",
    "        \n",
    "    train_views = list(train_set.to('cpu'))\n",
    "    val_views = list(val_set.to('cpu'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(dgcca.parameters(), lr=run.lr)\n",
    "    num_batches = len(train_views[0])//run.batch_size\n",
    "    \n",
    "    criterion = run.loss_func()\n",
    "\n",
    "    out = torch.stack(dgcca(train_views))\n",
    "    G = g_step(out.clone().detach())  \n",
    "    \n",
    "    M_serv = out.detach().clone()\n",
    "    \n",
    "    I = len(train_views)\n",
    "    \n",
    "    for epoch in range(60):\n",
    "        total_recons_loss = 0\n",
    "        total_val_loss = 0\n",
    "        batch_count = 0\n",
    "        total_ae_loss = 0\n",
    "        total_dgcca_loss = 0\n",
    "        \n",
    "        dgcca.train()\n",
    "        dgcca.to('cuda')\n",
    "        \n",
    "        for _ in trange(run.num_inner_epochs):\n",
    "            for i in trange(num_batches):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch = []\n",
    "\n",
    "                # mini batch gradient\n",
    "                batch = [view[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda') for view in train_views]            \n",
    "                target = G[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda')\n",
    "\n",
    "                latent = dgcca(batch)\n",
    "\n",
    "                ae_loss = (run.latent_dim/(2*28*28*target.shape[0]))*torch.norm(torch.stack(dgcca.decode(latent)) - torch.stack(batch))\n",
    "                \n",
    "                dgcca_loss = 1/2*torch.norm(torch.stack(latent)-target)/target.shape[0] \n",
    "\n",
    "                loss = dgcca_loss + ae_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_recons_loss += loss.item()\n",
    "                total_ae_loss += ae_loss.item()\n",
    "                total_dgcca_loss += dgcca_loss.item()\n",
    "                del batch, target, latent\n",
    "        \n",
    "        dgcca.eval()\n",
    "        dgcca.to('cpu')\n",
    "        out = torch.stack(dgcca(train_views)).detach().clone()        \n",
    "        if run.quant:\n",
    "            for i in range(I):\n",
    "                diff = out[i] - M_serv[i]\n",
    "                max_val = diff.abs().max()\n",
    "                quant = ((1/max_val)*diff[i]).round()*(max_val/1)\n",
    "                var = M_serv[i] + quant\n",
    "                M_serv[i] = var\n",
    "                del max_val, diff, quant, var\n",
    "            G = g_step(M_serv.clone().detach())\n",
    "        else:\n",
    "            G = g_step(out.clone().detach())   \n",
    "            \n",
    "        # validation loss\n",
    "        out_val = dgcca(val_views)\n",
    "        out_val = torch.stack(out_val)\n",
    "        \n",
    "        G_val = g_step(out_val.clone().detach())\n",
    "        \n",
    "        \n",
    "        loss_val = 1/2*torch.norm(out_val-G_val)/G_val.shape[0]\n",
    "        total_val_loss += loss_val.item()\n",
    "        del out, G_val, out_val\n",
    "\n",
    "        \n",
    "        results = OrderedDict()\n",
    "        results['epoch'] = epoch\n",
    "        results['total_loss'] = total_recons_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['ae_loss'] = total_ae_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['dgcca_loss'] = total_dgcca_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['val_fidelity'] = total_val_loss\n",
    "        results['batch_size'] = run.batch_size\n",
    "        results['lr'] = run.lr\n",
    "        \n",
    "#         if results['data_fidelity'] < 0.00275:\n",
    "#             break\n",
    "        \n",
    "        run_data.append(results)\n",
    "        df3 = pd.DataFrame.from_dict(run_data, orient='columns')\n",
    "        clear_output(wait=True)\n",
    "#         show_latent()\n",
    "        display(df3)\n",
    "            \n",
    "#             m.track_loss(G_adv_loss=losses['beta_kl-divergence'], G_mse_loss=losses[''], D_real_loss=total_D_real, D_fake_loss=total_D_fake, D_real_count=real_count, D_fake_count=fake_count)\n",
    "#         print(epoch, \"total_Gloss:\",total_Gloss, \"total_Dloss:\",total_Dloss, \"mse:\",total_mse_loss, \"adv: \", total_adv_loss)           \n",
    "#         m.end_epoch()\n",
    "        torch.save(dgcca, 'trained_models/dgcca_mnist_linear8_innerit1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "settled-suffering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>total_loss</th>\n",
       "      <th>ae_loss</th>\n",
       "      <th>dgcca_loss</th>\n",
       "      <th>val_fidelity</th>\n",
       "      <th>diff_norm</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.008885</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>145</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  total_loss   ae_loss  dgcca_loss  val_fidelity  diff_norm     lr\n",
       "0        0    0.009228  0.003430    0.008885      0.002895          0  0.001\n",
       "1        1    0.002521  0.003350    0.002186      0.003871          0  0.001\n",
       "2        2    0.002672  0.003348    0.002337      0.002911          0  0.001\n",
       "3        3    0.002440  0.003347    0.002105      0.002937          0  0.001\n",
       "4        4    0.002487  0.003347    0.002152      0.002918          0  0.001\n",
       "..     ...         ...       ...         ...           ...        ...    ...\n",
       "145    145    0.000399  0.003346    0.000065      0.002410          0  0.001\n",
       "146    146    0.000398  0.003346    0.000064      0.002410          0  0.001\n",
       "147    147    0.000398  0.003346    0.000063      0.002410          0  0.001\n",
       "148    148    0.000397  0.003346    0.000063      0.002410          0  0.001\n",
       "149    149    0.000397  0.003346    0.000063      0.002411          0  0.001\n",
       "\n",
       "[150 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from dataset import get_mnist_dataset\n",
    "# train_set, train_classes = get_mnist_dataset(train=True, shuffle=True, normalize=False)\n",
    "# val_set, val_classes = get_mnist_dataset(train=False, total_data=1000, shuffle=True, normalize=False)\n",
    "\n",
    "# train_set = train_set.view(train_set.shape[0], train_set.shape[1], -1)\n",
    "# val_set = val_set.view(val_set.shape[0], val_set.shape[1], -1)\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import product\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.run_manager import RunBuilder\n",
    "from models import g_step, MnistAEDGCCA, MnistAELinear, MnistAELinearBN\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from dataset import get_mnist_dataset \n",
    "import numpy as np\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    devices = ['cpu']\n",
    "else:\n",
    "    devices = ['cpu']\n",
    "print('starting')\n",
    "\n",
    "\n",
    "params = OrderedDict(\n",
    "    lr = [0.001],\n",
    "    batch_size = [1000],\n",
    "    device = devices,\n",
    "    shuffle = [True],\n",
    "    num_workers = [5],\n",
    "    manual_seed = [1265],\n",
    "    loss_func = [nn.MSELoss],\n",
    "    quant = [False],\n",
    "    latent_dim = [10], \n",
    "    num_inner_epochs = [1]\n",
    ")\n",
    "\n",
    "# layer_sizes_list = 3*[[128, 64, 2]]\n",
    "# input_size_list = 3*[2]\n",
    "\n",
    "\n",
    "run_count = 0\n",
    "models = []\n",
    "\n",
    "\n",
    "run_data = []\n",
    "\n",
    "data_load_time = 0\n",
    "forward_time = 0\n",
    "n_levels = 1\n",
    "diff_norm = 0\n",
    "\n",
    "for run in RunBuilder.get_runs(params):\n",
    "#     torch.cuda.set_device(run.device)\n",
    "    \n",
    "    run_count += 1\n",
    "    device = torch.device(run.device)\n",
    "    \n",
    "    dgcca = MnistAEDGCCA(output_size=run.latent_dim, network=MnistAELinear)\n",
    "    dgcca = dgcca.to('cpu')\n",
    "        \n",
    "    train_views = list(train_set.to('cpu'))\n",
    "    val_views = list(val_set.to('cpu'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(dgcca.parameters(), lr=run.lr)\n",
    "    scheduler = MultiStepLR(optimizer, [30,70], gamma=0.8)\n",
    "    num_batches = len(train_views[0])//run.batch_size\n",
    "    \n",
    "    criterion = run.loss_func()\n",
    "\n",
    "    out = torch.stack(dgcca(train_views))\n",
    "    G = g_step(out.clone().detach())  \n",
    "    \n",
    "    M_serv = out.detach().clone()\n",
    "    \n",
    "    I = len(train_views)\n",
    "    \n",
    "    for epoch in range(150):\n",
    "        total_recons_loss = 0\n",
    "        total_val_loss = 0\n",
    "        batch_count = 0\n",
    "        total_ae_loss = 0\n",
    "        total_dgcca_loss = 0\n",
    "        \n",
    "        dgcca.train()\n",
    "        dgcca.to('cuda')\n",
    "        \n",
    "        for _ in trange(run.num_inner_epochs):\n",
    "            for i in trange(num_batches):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch = []\n",
    "\n",
    "                # mini batch gradient\n",
    "                batch = [view[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda') for view in train_views]            \n",
    "                target = G[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda')\n",
    "\n",
    "                latent = dgcca(batch)\n",
    "\n",
    "                ae_loss = (run.latent_dim/(2*28*28*target.shape[0]))*torch.norm(torch.stack(dgcca.decode(latent)) - torch.stack(batch))\n",
    "                \n",
    "                dgcca_loss = 1/2*torch.norm(torch.stack(latent)-target)/target.shape[0] \n",
    "\n",
    "                loss = dgcca_loss + 0.1*ae_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_recons_loss += loss.item()\n",
    "                total_ae_loss += ae_loss.item()\n",
    "                total_dgcca_loss += dgcca_loss.item()\n",
    "                del batch, target, latent\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        dgcca.eval()\n",
    "        dgcca.to('cpu')\n",
    "        out = torch.stack(dgcca(train_views)).detach().clone()        \n",
    "        if run.quant:\n",
    "            for i in range(I):\n",
    "                diff = out[i] - M_serv[i]\n",
    "                max_val = diff.abs().max()\n",
    "                quant = ((1/max_val)*diff[i]).round()*(max_val/1)\n",
    "                var = M_serv[i] + quant\n",
    "                M_serv[i] = var\n",
    "                diff_norm = torch.norm(diff, 'fro')\n",
    "                del max_val, diff, quant, var\n",
    "            G = g_step(M_serv.clone().detach())\n",
    "        else:\n",
    "            G = g_step(out.clone().detach())   \n",
    "            \n",
    "        # validation loss\n",
    "        out_val = dgcca(val_views)\n",
    "        out_val = torch.stack(out_val)\n",
    "        \n",
    "        G_val = g_step(out_val.clone().detach())\n",
    "        \n",
    "        \n",
    "        loss_val = 1/2*torch.norm(out_val-G_val)/G_val.shape[0]\n",
    "        total_val_loss += loss_val.item()\n",
    "        del out, G_val, out_val\n",
    "\n",
    "        \n",
    "        results = OrderedDict()\n",
    "        results['epoch'] = epoch\n",
    "        results['total_loss'] = total_recons_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['ae_loss'] = total_ae_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['dgcca_loss'] = total_dgcca_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['val_fidelity'] = total_val_loss\n",
    "        results['diff_norm'] = diff_norm\n",
    "        results['lr'] = run.lr\n",
    "        \n",
    "#         if results['data_fidelity'] < 0.00275:\n",
    "#             break\n",
    "        \n",
    "        run_data.append(results)\n",
    "        df3 = pd.DataFrame.from_dict(run_data, orient='columns')\n",
    "        clear_output(wait=True)\n",
    "#         show_latent()\n",
    "        display(df3)\n",
    "        df3.to_pickle('plt/fed_linear12.pkl')\n",
    "        torch.save(dgcca, 'trained_models/dgcca_mnist_linear12_innerit1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desperate-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_pickle('plt/fed_linearBN10.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "molecular-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_fed = pd.read_pickle('plt/fed_linear.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "posted-formation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3c89f79664d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dgcca_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dgcca_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df3' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(df3['dgcca_loss'])\n",
    "plt.plot(df_fed['dgcca_loss'])\n",
    "\n",
    "plt.legend(['DGCCA', 'Federated DGCCA'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bright-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('plt/july9_loss_dgcca_log.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-joseph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "facial-astrology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa4b234eac8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df3['dgcca_loss'])\n",
    "plt.plot(df_fed['dgcca_loss'])\n",
    "\n",
    "plt.legend(['DGCCA', 'Federated DGCCA'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
