{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "printable-fence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>dgcca_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  dgcca_loss  val_loss  batch_size     lr\n",
       "0       0    0.003766    0.000414  0.000882        1000  0.001\n",
       "1       1    0.003490    0.000308  0.000871        1000  0.001\n",
       "2       2    0.003234    0.000321  0.000890        1000  0.001\n",
       "3       3    0.003139    0.000323  0.000891        1000  0.001\n",
       "4       4    0.003084    0.000323  0.000893        1000  0.001\n",
       "5       5    0.003049    0.000322  0.000895        1000  0.001\n",
       "6       6    0.003023    0.000322  0.000895        1000  0.001\n",
       "7       7    0.003004    0.000322  0.000897        1000  0.001\n",
       "8       8    0.002989    0.000321  0.000898        1000  0.001\n",
       "9       9    0.002977    0.000321  0.000899        1000  0.001\n",
       "10     10    0.002967    0.000321  0.000900        1000  0.001\n",
       "11     11    0.002959    0.000320  0.000901        1000  0.001\n",
       "12     12    0.002951    0.000320  0.000902        1000  0.001\n",
       "13     13    0.002945    0.000320  0.000902        1000  0.001\n",
       "14     14    0.002940    0.000320  0.000902        1000  0.001\n",
       "15     15    0.002935    0.000320  0.000903        1000  0.001\n",
       "16     16    0.002930    0.000320  0.000902        1000  0.001\n",
       "17     17    0.002926    0.000320  0.000904        1000  0.001\n",
       "18     18    0.002921    0.000320  0.000904        1000  0.001\n",
       "19     19    0.003583    0.000416  0.000903        1000  0.001\n",
       "20     20    0.003052    0.000321  0.000909        1000  0.001\n",
       "21     21    0.003011    0.000319  0.000908        1000  0.001\n",
       "22     22    0.002992    0.000319  0.000908        1000  0.001\n",
       "23     23    0.002976    0.000319  0.000908        1000  0.001\n",
       "24     24    0.002963    0.000319  0.000907        1000  0.001\n",
       "25     25    0.002952    0.000319  0.000906        1000  0.001\n",
       "26     26    0.002942    0.000319  0.000905        1000  0.001\n",
       "27     27    0.002934    0.000319  0.000906        1000  0.001\n",
       "28     28    0.002928    0.000319  0.000904        1000  0.001\n",
       "29     29    0.002921    0.000319  0.000905        1000  0.001\n",
       "30     30    0.002915    0.000319  0.000906        1000  0.001\n",
       "31     31    0.002910    0.000319  0.000906        1000  0.001\n",
       "32     32    0.002906    0.000319  0.000906        1000  0.001\n",
       "33     33    0.002902    0.000319  0.000907        1000  0.001\n",
       "34     34    0.002899    0.000319  0.000906        1000  0.001\n",
       "35     35    0.002896    0.000318  0.000907        1000  0.001\n",
       "36     36    0.002894    0.000319  0.000907        1000  0.001\n",
       "37     37    0.002889    0.000318  0.000908        1000  0.001\n",
       "38     38    0.002887    0.000318  0.000908        1000  0.001\n",
       "39     39    0.002884    0.000318  0.000908        1000  0.001\n",
       "40     40    0.004152    0.000521  0.000900        1000  0.001\n",
       "41     41    0.003055    0.000326  0.000904        1000  0.001\n",
       "42     42    0.003003    0.000321  0.000907        1000  0.001\n",
       "43     43    0.003496    0.000444  0.000908        1000  0.001\n",
       "44     44    0.003238    0.000317  0.000901        1000  0.001\n",
       "45     45    0.004110    0.000405  0.000917        1000  0.001\n",
       "46     46    0.003238    0.000334  0.000920        1000  0.001\n",
       "47     47    0.003185    0.000331  0.000920        1000  0.001\n",
       "48     48    0.003159    0.000329  0.000920        1000  0.001\n",
       "49     49    0.003140    0.000327  0.000920        1000  0.001\n",
       "50     50    0.003124    0.000326  0.000920        1000  0.001\n",
       "51     51    0.003108    0.000326  0.000919        1000  0.001\n",
       "52     52    0.003091    0.000326  0.000919        1000  0.001\n",
       "53     53    0.003072    0.000326  0.000918        1000  0.001\n",
       "54     54    0.003051    0.000326  0.000917        1000  0.001\n",
       "55     55    0.003029    0.000327  0.000917        1000  0.001\n",
       "56     56    0.003009    0.000327  0.000917        1000  0.001\n",
       "57     57    0.002993    0.000327  0.000916        1000  0.001\n",
       "58     58    0.002978    0.000327  0.000917        1000  0.001\n",
       "59     59    0.002967    0.000326  0.000916        1000  0.001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset import get_mnist_dataset\n",
    "train_set, train_classes = get_mnist_dataset(train=True, shuffle=True, normalize=False)\n",
    "val_set, val_classes = get_mnist_dataset(train=False, total_data=1000, shuffle=True, normalize=False)\n",
    "\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import product\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.run_manager import RunBuilder\n",
    "from models import g_step, MnistAEDGCCA, MnistAutoencoderNoBN\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from dataset import get_mnist_dataset \n",
    "import numpy as np\n",
    "\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    devices = ['cpu']\n",
    "else:\n",
    "    devices = ['cpu']\n",
    "print('starting')\n",
    "\n",
    "\n",
    "params = OrderedDict(\n",
    "    lr = [0.001],\n",
    "    batch_size = [1000],\n",
    "    device = devices,\n",
    "    shuffle = [True],\n",
    "    num_workers = [5],\n",
    "    manual_seed = [1265],\n",
    "    loss_func = [nn.MSELoss],\n",
    "    quant = [True],\n",
    "    latent_dim = [10], \n",
    "    num_inner_epochs = [8]\n",
    ")\n",
    "\n",
    "# layer_sizes_list = 3*[[128, 64, 2]]\n",
    "# input_size_list = 3*[2]\n",
    "\n",
    "\n",
    "run_count = 0\n",
    "models = []\n",
    "\n",
    "\n",
    "run_data = []\n",
    "\n",
    "data_load_time = 0\n",
    "forward_time = 0\n",
    "\n",
    "\n",
    "for run in RunBuilder.get_runs(params):\n",
    "#     torch.cuda.set_device(run.device)\n",
    "    \n",
    "    run_count += 1\n",
    "    device = torch.device(run.device)\n",
    "    \n",
    "    dgcca = MnistAEDGCCA(output_size=run.latent_dim, network=)\n",
    "    dgcca = dgcca.to('cpu')\n",
    "        \n",
    "    train_views = list(train_set.to('cpu'))\n",
    "    val_views = list(val_set.to('cpu'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(dgcca.parameters(), lr=run.lr)\n",
    "    num_batches = len(train_views[0])//run.batch_size\n",
    "    \n",
    "    criterion = run.loss_func()\n",
    "\n",
    "    out = torch.stack(dgcca(train_views))\n",
    "    G = g_step(out.clone().detach())  \n",
    "    \n",
    "    M_serv = out.detach().clone()\n",
    "    \n",
    "    I = len(train_views)\n",
    "    \n",
    "    for epoch in range(60):\n",
    "        total_recons_loss = 0\n",
    "        total_val_loss = 0\n",
    "        batch_count = 0\n",
    "        total_dgcca_loss = 0\n",
    "        \n",
    "        dgcca.to('cuda')\n",
    "        \n",
    "        for _ in tqdm(range(run.num_inner_epochs)):\n",
    "            for i in tqdm(range(num_batches)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch = []\n",
    "\n",
    "                # mini batch gradient\n",
    "                batch = [view[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda') for view in train_views]            \n",
    "                target = G[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda')\n",
    "\n",
    "                latent = dgcca(batch)\n",
    "\n",
    "                ae_loss = (run.latent_dim/(2*28*28*target.shape[0]))*torch.norm(torch.stack(dgcca.decode(latent)) - torch.stack(batch))\n",
    "\n",
    "                dgcca_loss = 1/2*torch.norm(torch.stack(latent)-target)/target.shape[0] \n",
    "\n",
    "                loss = dgcca_loss + ae_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_recons_loss += loss.item()\n",
    "                total_dgcca_loss += dgcca_loss.item()\n",
    "#                 print(ae_loss.item(), dgcca_loss.item())\n",
    "                del batch, target, latent\n",
    "\n",
    "        dgcca.to('cpu')\n",
    "        out = torch.stack(dgcca(train_views)).detach().clone()        \n",
    "        if run.quant:\n",
    "            for i in range(I):\n",
    "                diff = out[i] - M_serv[i]\n",
    "                max_val = diff.abs().max()\n",
    "                quant = ((1/max_val)*diff[i]).round()*(max_val/1)\n",
    "                var = M_serv[i] + quant\n",
    "                M_serv[i] = var\n",
    "                del max_val, diff, quant, var\n",
    "            G = g_step(M_serv.clone().detach())\n",
    "        else:\n",
    "            G = g_step(out.clone().detach())   \n",
    "            \n",
    "        # validation loss\n",
    "        out_val = dgcca(val_views)\n",
    "        out_val = torch.stack(out_val)\n",
    "        \n",
    "        G_val = g_step(out_val.clone().detach())\n",
    "        \n",
    "        loss_val = criterion(out_val, G_val)\n",
    "        total_val_loss += loss_val.item()\n",
    "        del out, G_val, out_val\n",
    "\n",
    "        \n",
    "        results = OrderedDict()\n",
    "        results['epoch'] = epoch\n",
    "        results['train_loss'] = total_recons_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['dgcca_loss'] = total_dgcca_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['val_loss'] = total_val_loss\n",
    "        results['batch_size'] = run.batch_size\n",
    "        results['lr'] = run.lr\n",
    "        if results['train_loss'] < 0.00275:\n",
    "            break\n",
    "        run_data.append(results)\n",
    "        df3 = pd.DataFrame.from_dict(run_data, orient='columns')\n",
    "        clear_output(wait=True)\n",
    "#         show_latent()\n",
    "        display(df3)\n",
    "            \n",
    "#             m.track_loss(G_adv_loss=losses['beta_kl-divergence'], G_mse_loss=losses[''], D_real_loss=total_D_real, D_fake_loss=total_D_fake, D_real_count=real_count, D_fake_count=fake_count)\n",
    "#         print(epoch, \"total_Gloss:\",total_Gloss, \"total_Dloss:\",total_Dloss, \"mse:\",total_mse_loss, \"adv: \", total_adv_loss)           \n",
    "#         m.end_epoch()\n",
    "        torch.save(dgcca, 'trained_models/dgcca_mnist_ae2_fed.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sixth-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/torch/nn/functional.py:3888: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/ipykernel_launcher.py:103: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1060b6caddc44127aa4f6fbf2c47a494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sagar/Projects/learn/compressai/venv/lib/python3.6/site-packages/ipykernel_launcher.py:104: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58882d24e044cc384f7aabdafaca0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (784) must match the size of tensor b (28) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c4a4fb6936a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdgcca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mae_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdgcca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mdgcca_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (784) must match the size of tensor b (28) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "from dataset import get_mnist_dataset\n",
    "train_set, train_classes = get_mnist_dataset(train=True, shuffle=True, normalize=False)\n",
    "val_set, val_classes = get_mnist_dataset(train=False, total_data=1000, shuffle=True, normalize=False)\n",
    "\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import product\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.run_manager import RunBuilder\n",
    "from models import g_step, MnistAEDGCCA, MnistAELinear\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from dataset import get_mnist_dataset \n",
    "import numpy as np\n",
    "\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    devices = ['cpu']\n",
    "else:\n",
    "    devices = ['cpu']\n",
    "print('starting')\n",
    "\n",
    "\n",
    "params = OrderedDict(\n",
    "    lr = [0.001],\n",
    "    batch_size = [1000],\n",
    "    device = devices,\n",
    "    shuffle = [True],\n",
    "    num_workers = [5],\n",
    "    manual_seed = [1265],\n",
    "    loss_func = [nn.MSELoss],\n",
    "    quant = [False],\n",
    "    latent_dim = [10], \n",
    "    num_inner_epochs = [1]\n",
    ")\n",
    "\n",
    "# layer_sizes_list = 3*[[128, 64, 2]]\n",
    "# input_size_list = 3*[2]\n",
    "\n",
    "\n",
    "run_count = 0\n",
    "models = []\n",
    "\n",
    "\n",
    "run_data = []\n",
    "\n",
    "data_load_time = 0\n",
    "forward_time = 0\n",
    "\n",
    "\n",
    "for run in RunBuilder.get_runs(params):\n",
    "#     torch.cuda.set_device(run.device)\n",
    "    \n",
    "    run_count += 1\n",
    "    device = torch.device(run.device)\n",
    "    \n",
    "    dgcca = MnistAEDGCCA(output_size=run.latent_dim, network=MnistAELinear)\n",
    "    dgcca = dgcca.to('cpu')\n",
    "        \n",
    "    train_views = list(train_set.to('cpu'))\n",
    "    val_views = list(val_set.to('cpu'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(dgcca.parameters(), lr=run.lr)\n",
    "    num_batches = len(train_views[0])//run.batch_size\n",
    "    \n",
    "    criterion = run.loss_func()\n",
    "\n",
    "    out = torch.stack(dgcca(train_views))\n",
    "    G = g_step(out.clone().detach())  \n",
    "    \n",
    "    M_serv = out.detach().clone()\n",
    "    \n",
    "    I = len(train_views)\n",
    "    \n",
    "    for epoch in range(60):\n",
    "        total_recons_loss = 0\n",
    "        total_val_loss = 0\n",
    "        batch_count = 0\n",
    "        total_dgcca_loss = 0\n",
    "        \n",
    "        dgcca.to('cuda')\n",
    "        \n",
    "        for _ in tqdm(range(run.num_inner_epochs)):\n",
    "            for i in tqdm(range(num_batches)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch = []\n",
    "\n",
    "                # mini batch gradient\n",
    "                batch = [view[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda') for view in train_views]            \n",
    "                target = G[(i*run.batch_size):((i+1)*run.batch_size), :].to('cuda')\n",
    "\n",
    "                latent = dgcca(batch)\n",
    "\n",
    "                ae_loss = (run.latent_dim/(2*28*28*target.shape[0]))*torch.norm(torch.stack(dgcca.decode(latent)) - torch.stack(batch))\n",
    "\n",
    "                dgcca_loss = 1/2*torch.norm(torch.stack(latent)-target)/target.shape[0] \n",
    "\n",
    "                loss = dgcca_loss + ae_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_recons_loss += loss.item()\n",
    "                total_dgcca_loss += dgcca_loss.item()\n",
    "#                 print(ae_loss.item(), dgcca_loss.item())\n",
    "                del batch, target, latent\n",
    "\n",
    "        dgcca.to('cpu')\n",
    "        out = torch.stack(dgcca(train_views)).detach().clone()        \n",
    "        if run.quant:\n",
    "            for i in range(I):\n",
    "                diff = out[i] - M_serv[i]\n",
    "                max_val = diff.abs().max()\n",
    "                quant = ((1/max_val)*diff[i]).round()*(max_val/1)\n",
    "                var = M_serv[i] + quant\n",
    "                M_serv[i] = var\n",
    "                del max_val, diff, quant, var\n",
    "            G = g_step(M_serv.clone().detach())\n",
    "        else:\n",
    "            G = g_step(out.clone().detach())   \n",
    "            \n",
    "        # validation loss\n",
    "        out_val = dgcca(val_views)\n",
    "        out_val = torch.stack(out_val)\n",
    "        \n",
    "        G_val = g_step(out_val.clone().detach())\n",
    "        \n",
    "        loss_val = criterion(out_val, G_val)\n",
    "        total_val_loss += loss_val.item()\n",
    "        del out, G_val, out_val\n",
    "\n",
    "        \n",
    "        results = OrderedDict()\n",
    "        results['epoch'] = epoch\n",
    "        results['train_loss'] = total_recons_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['dgcca_loss'] = total_dgcca_loss/(num_batches*run.num_inner_epochs)\n",
    "        results['val_loss'] = total_val_loss\n",
    "        results['batch_size'] = run.batch_size\n",
    "        results['lr'] = run.lr\n",
    "        if results['train_loss'] < 0.00290:\n",
    "            break\n",
    "        run_data.append(results)\n",
    "        df4 = pd.DataFrame.from_dict(run_data, orient='columns')\n",
    "        clear_output(wait=True)\n",
    "#         show_latent()\n",
    "        display(df4)\n",
    "            \n",
    "#             m.track_loss(G_adv_loss=losses['beta_kl-divergence'], G_mse_loss=losses[''], D_real_loss=total_D_real, D_fake_loss=total_D_fake, D_real_count=real_count, D_fake_count=fake_count)\n",
    "#         print(epoch, \"total_Gloss:\",total_Gloss, \"total_Dloss:\",total_Dloss, \"mse:\",total_mse_loss, \"adv: \", total_adv_loss)           \n",
    "#         m.end_epoch()\n",
    "        torch.save(dgcca, 'trained_models/dgcca_mnist_linear_innerit=1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "increased-dubai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ideal-application",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
